# Red Teaming Portfolio

This repository documents structured red teaming work focused on large language model (LLM) reliability, safety testing, and evaluation design.

The emphasis is on:
- Adversarial prompt testing (sanitized)
- Rubric-based evaluation and calibration
- Consistency and regression analysis
- Practical mitigation strategies

All artifacts are intentionally sanitized to prevent misuse while preserving technical insight and methodological rigor.


* **Writeups**: clear problem framing, approach, and outcomes
* **Artifacts**: sanitized prompts, notes, and reports
* **Evals**: rubric-based reasoning and consistency checks

* ## Featured Artifact

- [Artifact 001 â€” Rubric Calibration & Consistency Analysis](artifacts/001-rubric-calibration.md)

  ---

## Professional Notes

- All examples are sanitized and designed to avoid enabling misuse.
- Focus is on methodology, reproducibility, and reliability under variation.
- This repository reflects ongoing structured experimentation in LLM safety and evaluation design.

For collaboration or discussion, feel free to connect via GitHub.

