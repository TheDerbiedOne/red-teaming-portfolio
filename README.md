# Red Teaming Portfolio

This repo contains my red teaming work focused on LLM safety testing: prompt attack/defense, evaluation design, and rubric-based analysis. I share writeups and *sanitized* examples that demonstrate methodology, findings, and practical lessons—without exposing proprietary data or enabling misuse.

## What you'll find here (coming next)

* **Writeups**: clear problem framing, approach, and outcomes
* **Artifacts**: sanitized prompts, notes, and reports
* **Evals**: rubric-based reasoning and consistency checks

* ## Featured Artifact

- [Artifact 001 — Rubric Calibration & Consistency Analysis](artifacts/001-rubric-calibration.md)

